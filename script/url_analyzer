#!/usr/bin/env ruby
require 'rubygems'
require 'wukong'
require 'bundler/setup'
require 'optparse'
$LOAD_PATH.unshift(File.expand_path('../lib', File.dirname(__FILE__)))
require 'mjg_twitter_tools'
# require File.dirname(__FILE__)+'/../lib/mjg_twitter_tools'

options = {}
parser = OptionParser.new do|opts|
  opts.banner = "Usage: url_analyzer [options]"
  opts.on('--data_directory data_directory', 'Directory to download twitter activities into') do |data_directory|
    options[:data_directory] = data_directory
  end
  opts.on('-s', '--start start_date', 'First date to generate URL reports for') do |start_date|
    options[:start_date] = start_date
  end
  opts.on('-e', '--end end_date', 'Last date to generate URL reports for') do |end_date|
    options[:end_date] = end_date
  end
  opts.on('-f','--full', 'Perform a full run: download, extract and publish') do
    options[:download] = true
    options[:extract] = true
    options[:reduce] = true
    options[:dat] = true
    options[:publish] = true
  end
  opts.on('-d','--download', 'Download new source data from GNIP') do
    options[:download] = true
  end
  opts.on('-x', '--extract', 'Generate new URL reports from the data') do
    options[:extract] = true
  end
  opts.on('--reduce', 'Merge the results with existing dataset and reduce it to a consolidated dataset') do
    options[:reduce] = true
  end
  opts.on('--dat', 'Commit the data to the dat dataset stored in ./dat_repo') do
    options[:dat] = true
  end
  opts.on('--analysis-only', 'Only perform analysis tasks: Extract and Reduce') do
    options[:extract] = true
    options[:reduce] = true
  end
  opts.on('-p','--publish', 'Publish Reports to DataBindery') do
    options[:publish] = true
  end
  opts.on('-r','--rejects', 'Generate Reports of Rejected URLs') do
    options[:rejects] = true
  end
  opts.on('--no-dat', 'Don\'t read/write data to or from the local dat repository when running the dataset reducer.') do
    options[:using_dat] = false
  end
end
parser.parse!

start_date = options.fetch(:start_date, (Date.today-1).strftime("%Y%m%d"))   
end_date = options.fetch(:end_date, start_date) 
data_directory = options.fetch(:data_directory, start_date)  
using_dat = options.fetch(:using_dat, false)

downloader = Gnip::SearchResultsDownloader.new(start_date:start_date,end_date:end_date,data_directory:data_directory)

if options[:download]
  downloader.download_latest_results
end

extractor = ExtractionRunner.new(source_dir_path:downloader.output_dir_path, start_date:start_date, end_date:end_date)
if options[:extract]
  extractor.process
  puts extractor.message
end

dat_repository = Dat::Repository.new(dir: 'dat_repo') if using_dat

reducer = UrlDatasetReducerRunner.new(source_dir_path:downloader.output_dir_path, start_date:start_date, end_date:end_date, dat_repository: dat_repository)
if options[:reduce]
  reducer.process
  puts reducer.message
end

if options[:publish]
  start_at_commit ||= nil
  ReportPublisher.publish_from_dat(dat_repository, start_at: start_at_commit)
end

if options[:rejects]
  rejects_extractor = ExtractionRunner.new(source_dir_path:downloader.output_dir_path, start_date:start_date, end_date:end_date, invert_filters:true)
  rejects_extractor.process
  puts rejects_extractor.message
  #publisher.publish_reports(rejects_extractor.processed_reports)
  #puts publisher.message
end
